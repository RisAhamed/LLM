{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the token IDs for a sentence (Input)\n",
    "token_ids = torch.tensor([101, 204, 333, 410, 101, 897])  # Example token IDs\n",
    "\n",
    "# Step 1: Embedding Layer (Input: Token IDs)\n",
    "vocab_size = 1000  # Assuming we have a vocabulary of 1000 tokens\n",
    "embedding_dim = 512  # Size of each embedding vector\n",
    "\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "word_embeddings = embedding_layer(token_ids)  # Output: Embeddings for each token\n",
    "\n",
    "# Check the shape of the output (it should be [Sequence Length, Embedding Dimension])\n",
    "print(word_embeddings.shape)  # Output: torch.Size([6, 512]) for 6 tokens\n",
    "\n",
    "# Step 2: Add Positional Encoding (if necessary)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embedding_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "pos_enc = PositionalEncoding(embedding_dim)\n",
    "word_embeddings_with_positional_encoding = pos_enc(word_embeddings)\n",
    "\n",
    "# Final output with positional encoding\n",
    "print(word_embeddings_with_positional_encoding.shape)  # Output: torch.Size([6, 512])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer.shape)\n",
    "print(word_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = torch.arange(0,50000, dtype=torch.float)\n",
    "print(position)\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "print(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Xray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
