{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.13.1-cp37-cp37m-win_amd64.whl (162.6 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\riswa\\anaconda3\\envs\\rstudio\\lib\\site-packages (from torch) (4.7.1)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.13.1\n"
     ]
    }
   ],
   "source": [
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\riswa\\anaconda3\\envs\\rstudio\\lib\\site-packages (1.13.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.14.1-cp37-cp37m-win_amd64.whl (1.1 MB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-0.13.1-cp37-cp37m-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\riswa\\anaconda3\\envs\\rstudio\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\riswa\\anaconda3\\envs\\rstudio\\lib\\site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: requests in c:\\users\\riswa\\anaconda3\\envs\\rstudio\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\riswa\\anaconda3\\envs\\rstudio\\lib\\site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\riswa\\anaconda3\\envs\\rstudio\\lib\\site-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\riswa\\anaconda3\\envs\\rstudio\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\riswa\\anaconda3\\envs\\rstudio\\lib\\site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\riswa\\anaconda3\\envs\\rstudio\\lib\\site-packages (from requests->torchvision) (3.6)\n",
      "Installing collected packages: torchvision, torchaudio\n",
      "Successfully installed torchaudio-0.13.1 torchvision-0.14.1\n"
     ]
    }
   ],
   "source": [
    "! pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape\n",
    "attention_score  =torch.empty(inputs.shape[0])\n",
    "for i,x in enumerate(inputs):\n",
    "    attention_score[i] = torch.dot(x,inputs[1])\n",
    "# attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5500, 0.8700, 0.6600])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4754)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(inputs[1] ,inputs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9544)\n",
      "tensor(1.4950)\n",
      "tensor(1.4754)\n",
      "tensor(0.8434)\n",
      "tensor(0.7070)\n",
      "tensor(1.0865)\n"
     ]
    }
   ],
   "source": [
    "attention_score = torch.ones(inputs.shape[0])\n",
    "query = inputs[1]\n",
    "for i,x_1  in enumerate(inputs):\n",
    "    attention_score[i] = torch.dot(x_1,query)\n",
    "    print(attention_score[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# attention_score\n",
    "print(query.shape,x_1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4300)\n",
      "tensor(0.1500)\n",
      "tensor(0.8900)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for dimension 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20212\\2603078570.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 3 is out of bounds for dimension 0 with size 3"
     ]
    }
   ],
   "source": [
    "attention_sc = torch.ones(inputs.shape[0])\n",
    "attention_sc\n",
    "for i,x in enumerate(inputs):\n",
    "    for j,k in enumerate( inputs):\n",
    "        print(inputs[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9544)\n",
      "tensor(1.4950)\n",
      "tensor(1.4754)\n",
      "tensor(0.8434)\n",
      "tensor(0.7070)\n",
      "tensor(1.0865)\n"
     ]
    }
   ],
   "source": [
    "for i in attention_score:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    \n",
    "    return torch.exp(x)/torch.exp(x).sum(dim =0)\n",
    "attention_sc_2 = softmax(attention_score)\n",
    "print(sum(attention_sc_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(attention_score,dim =0)\n",
    "print(attention_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(attention_weights.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5500, 0.8700, 0.6600])\n",
      "tensor([0., 0., 0.])\n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query  = inputs[1]\n",
    "context_vec= torch.zeros(query.shape)\n",
    "print(inputs[1])\n",
    "print(context_vec)\n",
    "for i,x_1 in enumerate(inputs):\n",
    "    context_vec += attention_weights[i]*x_1\n",
    "\n",
    "print(context_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 6]), tensor([0.0500, 0.8000, 0.5500]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.shape , x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4300, 0.1500, 0.8900],\n",
      "        [0.5500, 0.8700, 0.6600],\n",
      "        [0.5700, 0.8500, 0.6400],\n",
      "        [0.2200, 0.5800, 0.3300],\n",
      "        [0.7700, 0.2500, 0.1000],\n",
      "        [0.0500, 0.8000, 0.5500]])\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Computing attention weights for all input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4300, 0.1500, 0.8900]) tensor([0.4300, 0.1500, 0.8900])\n",
      "tensor([0.4300, 0.1500, 0.8900]) tensor([0.5500, 0.8700, 0.6600])\n",
      "tensor([0.4300, 0.1500, 0.8900]) tensor([0.5700, 0.8500, 0.6400])\n",
      "tensor([0.4300, 0.1500, 0.8900]) tensor([0.2200, 0.5800, 0.3300])\n",
      "tensor([0.4300, 0.1500, 0.8900]) tensor([0.7700, 0.2500, 0.1000])\n",
      "tensor([0.4300, 0.1500, 0.8900]) tensor([0.0500, 0.8000, 0.5500])\n",
      "tensor([0.5500, 0.8700, 0.6600]) tensor([0.4300, 0.1500, 0.8900])\n",
      "tensor([0.5500, 0.8700, 0.6600]) tensor([0.5500, 0.8700, 0.6600])\n",
      "tensor([0.5500, 0.8700, 0.6600]) tensor([0.5700, 0.8500, 0.6400])\n",
      "tensor([0.5500, 0.8700, 0.6600]) tensor([0.2200, 0.5800, 0.3300])\n",
      "tensor([0.5500, 0.8700, 0.6600]) tensor([0.7700, 0.2500, 0.1000])\n",
      "tensor([0.5500, 0.8700, 0.6600]) tensor([0.0500, 0.8000, 0.5500])\n",
      "tensor([0.5700, 0.8500, 0.6400]) tensor([0.4300, 0.1500, 0.8900])\n",
      "tensor([0.5700, 0.8500, 0.6400]) tensor([0.5500, 0.8700, 0.6600])\n",
      "tensor([0.5700, 0.8500, 0.6400]) tensor([0.5700, 0.8500, 0.6400])\n",
      "tensor([0.5700, 0.8500, 0.6400]) tensor([0.2200, 0.5800, 0.3300])\n",
      "tensor([0.5700, 0.8500, 0.6400]) tensor([0.7700, 0.2500, 0.1000])\n",
      "tensor([0.5700, 0.8500, 0.6400]) tensor([0.0500, 0.8000, 0.5500])\n",
      "tensor([0.2200, 0.5800, 0.3300]) tensor([0.4300, 0.1500, 0.8900])\n",
      "tensor([0.2200, 0.5800, 0.3300]) tensor([0.5500, 0.8700, 0.6600])\n",
      "tensor([0.2200, 0.5800, 0.3300]) tensor([0.5700, 0.8500, 0.6400])\n",
      "tensor([0.2200, 0.5800, 0.3300]) tensor([0.2200, 0.5800, 0.3300])\n",
      "tensor([0.2200, 0.5800, 0.3300]) tensor([0.7700, 0.2500, 0.1000])\n",
      "tensor([0.2200, 0.5800, 0.3300]) tensor([0.0500, 0.8000, 0.5500])\n",
      "tensor([0.7700, 0.2500, 0.1000]) tensor([0.4300, 0.1500, 0.8900])\n",
      "tensor([0.7700, 0.2500, 0.1000]) tensor([0.5500, 0.8700, 0.6600])\n",
      "tensor([0.7700, 0.2500, 0.1000]) tensor([0.5700, 0.8500, 0.6400])\n",
      "tensor([0.7700, 0.2500, 0.1000]) tensor([0.2200, 0.5800, 0.3300])\n",
      "tensor([0.7700, 0.2500, 0.1000]) tensor([0.7700, 0.2500, 0.1000])\n",
      "tensor([0.7700, 0.2500, 0.1000]) tensor([0.0500, 0.8000, 0.5500])\n",
      "tensor([0.0500, 0.8000, 0.5500]) tensor([0.4300, 0.1500, 0.8900])\n",
      "tensor([0.0500, 0.8000, 0.5500]) tensor([0.5500, 0.8700, 0.6600])\n",
      "tensor([0.0500, 0.8000, 0.5500]) tensor([0.5700, 0.8500, 0.6400])\n",
      "tensor([0.0500, 0.8000, 0.5500]) tensor([0.2200, 0.5800, 0.3300])\n",
      "tensor([0.0500, 0.8000, 0.5500]) tensor([0.7700, 0.2500, 0.1000])\n",
      "tensor([0.0500, 0.8000, 0.5500]) tensor([0.0500, 0.8000, 0.5500])\n"
     ]
    }
   ],
   "source": [
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        print(x_i,x_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = torch.softmax(attn_scores,dim = -1,)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (6) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20212\\1917774221.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mattn_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# attn_3 = torch.dot(inputs,inputs.T)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mattn_4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mattn_1\u001b[0m\u001b[1;33m==\u001b[0m \u001b[0mattn_2\u001b[0m \u001b[1;33m==\u001b[0m\u001b[0mattn_4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (6) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "attn_1 =inputs @ inputs.T\n",
    "attn_2 = torch.matmul(inputs , inputs.T)\n",
    "# attn_3 = torch.dot(inputs,inputs.T)\n",
    "\n",
    "attn_1== attn_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4646, 0.3685, 0.1669],\n",
      "        [0.5558, 0.2716, 0.1726]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "m = nn.Softmax(dim=-1)\n",
    "input = torch.randn(2, 3)\n",
    "output = m(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights= torch.softmax(attn_1,dim =-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "tensor([0.4421, 0.5931, 0.5790])\n"
     ]
    }
   ],
   "source": [
    "context_vec = attn_weights @ inputs \n",
    "print(context_vec.shape\n",
    "     )\n",
    "print(context_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the attention weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] # second input element\n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5500, 0.8700, 0.6600]), 3, 2)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2,d_in,d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]),\n",
       " Parameter containing:\n",
       " tensor([[0.2961, 0.5166],\n",
       "         [0.2517, 0.6886],\n",
       "         [0.0740, 0.8665]]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key.shape,W_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2,key_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4306, 1.4551])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " torch.matmul(x_2,W_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4361, 1.1156]),\n",
       " tensor([[0.1855, 0.8812],\n",
       "         [0.3951, 1.0037],\n",
       "         [0.3879, 0.9831],\n",
       "         [0.2393, 0.5493],\n",
       "         [0.1492, 0.3346],\n",
       "         [0.3221, 0.7863]]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys[2],values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8111)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[2]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_Score_2 = query_2 @ keys.T\n",
    "d_k = keys.shape[1]\n",
    "import math\n",
    "attn_weights_2 = torch.softmax(attn_Score_2 /math.sqrt(d_k),dim =-1 )\n",
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vec_2 = attn_weights_2 @ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3061, 0.8210])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Implementing a compact SelfAttention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0949, 0.9075],\n",
      "        [1.1209, 0.9305],\n",
      "        [1.1194, 0.9293],\n",
      "        [1.0644, 0.8803],\n",
      "        [1.0572, 0.8736],\n",
      "        [1.0846, 0.8986]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class selfAttention_v1(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_in,d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        keys =  x @ self.W_key\n",
    "        queries= x @ self.W_query\n",
    "        values = x@ self.W_value\n",
    "        attn_score =  queries @ keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "        attn_score / math.sqrt(keys.shape[-1]),dim =-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(1233)\n",
    "sa_v1 = selfAttention_v1(d_in,d_out)\n",
    "print(sa_v1(inputs))\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(d_in,d_out)\n",
    "d_in,d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1363,  0.4183],\n",
      "        [-0.1384,  0.4209],\n",
      "        [-0.1379,  0.4203],\n",
      "        [-0.1367,  0.4181],\n",
      "        [-0.1270,  0.4056],\n",
      "        [-0.1415,  0.4245]], grad_fn=<MmBackward0>)\n",
      "tensor([[0.1669, 0.1774, 0.1768, 0.1585, 0.1550, 0.1654],\n",
      "        [0.1675, 0.1802, 0.1793, 0.1566, 0.1500, 0.1664],\n",
      "        [0.1674, 0.1794, 0.1786, 0.1572, 0.1512, 0.1663],\n",
      "        [0.1676, 0.1756, 0.1750, 0.1601, 0.1544, 0.1673],\n",
      "        [0.1648, 0.1618, 0.1623, 0.1695, 0.1783, 0.1632],\n",
      "        [0.1685, 0.1836, 0.1823, 0.1544, 0.1426, 0.1686]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "# Now, let's create an instance of the class\n",
    "torch.manual_seed(1233)\n",
    "d_in, d_out = 3,2 #Define input and output dimensions\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "\n",
    "# # Define an example input\n",
    "# inputs = torch.tensor([[0.4, 0.5, 0.6, 0.7], \n",
    "#                        [0.1, 0.2, 0.3, 0.4], \n",
    "#                        [0.9, 1.0, 1.1, 1.2]])\n",
    "\n",
    "# Pass the input through the self-attention layer\n",
    "output = sa_v2(inputs)\n",
    "print(output)\n",
    "print(attn_v2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying a causal attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1669, 0.1774, 0.1768, 0.1585, 0.1550, 0.1654],\n",
      "        [0.1675, 0.1802, 0.1793, 0.1566, 0.1500, 0.1664],\n",
      "        [0.1674, 0.1794, 0.1786, 0.1572, 0.1512, 0.1663],\n",
      "        [0.1676, 0.1756, 0.1750, 0.1601, 0.1544, 0.1673],\n",
      "        [0.1648, 0.1618, 0.1623, 0.1695, 0.1783, 0.1632],\n",
      "        [0.1685, 0.1836, 0.1823, 0.1544, 0.1426, 0.1686]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_score = queries @ keys.T\n",
    "values = sa_v2.W_value(inputs)\n",
    "attn_weights = torch.softmax(attn_score/ math.sqrt(keys.shape[-1]),dim =-1)\n",
    "                   \n",
    "print(attn_weights)\n",
    "# context_vec = attn_weights  @ values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  23,    0],\n",
      "         [  24,   53],\n",
      "         [  23,   54]],\n",
      "\n",
      "        [[  23,    0],\n",
      "         [  43,   45],\n",
      "         [  64, 2345]]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask = torch.tril(torch.tensor([[[23,53],[24,53],[23,54]],\n",
    "                               [[23,53],[43,45],[64,2345]]]))\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "mask_sample = torch.tril(torch.ones(context_length,context_length))\n",
    "print(mask_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]])\n"
     ]
    }
   ],
   "source": [
    "row_sums = mask_sample.sum(dim =-1,keepdim =True)\n",
    "masked_simple_norm = mask_sample/ row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1669, 0.1774, 0.1768, 0.1585, 0.1550, 0.1654],\n",
       "        [0.1675, 0.1802, 0.1793, 0.1566, 0.1500, 0.1664],\n",
       "        [0.1674, 0.1794, 0.1786, 0.1572, 0.1512, 0.1663],\n",
       "        [0.1676, 0.1756, 0.1750, 0.1601, 0.1544, 0.1673],\n",
       "        [0.1648, 0.1618, 0.1623, 0.1695, 0.1783, 0.1632],\n",
       "        [0.1685, 0.1836, 0.1823, 0.1544, 0.1426, 0.1686]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.9544, 1.4950,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.9422, 1.4754, 1.4570,   -inf,   -inf,   -inf],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937,   -inf,   -inf],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654,   -inf],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# mask = torch.triu(torch.ones(context_length,context_length))\n",
    "# masked = attn_score.masked_fill(mask.bool(),-torch.inf)\n",
    "# print(masked)\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked/ math.sqrt(keys.shape[-1]),dim =-1)\n",
    "print(attn_weights.sum(dim =-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking additional attention weights with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1888, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.7482, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4352, 0.0000, 0.0000, 0.4409, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4357, 0.0000, 0.0000, 0.4230, 0.0000],\n",
      "        [0.2946, 0.4065, 0.0000, 0.2999, 0.2320, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "dropout = torch.nn.Dropout(0.5)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a compact causal self-attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.4300, 0.1500, 0.8900],\n",
       "          [0.5500, 0.8700, 0.6600],\n",
       "          [0.5700, 0.8500, 0.6400],\n",
       "          [0.2200, 0.5800, 0.3300],\n",
       "          [0.7700, 0.2500, 0.1000],\n",
       "          [0.0500, 0.8000, 0.5500]],\n",
       " \n",
       "         [[0.4300, 0.1500, 0.8900],\n",
       "          [0.5500, 0.8700, 0.6600],\n",
       "          [0.5700, 0.8500, 0.6400],\n",
       "          [0.2200, 0.5800, 0.3300],\n",
       "          [0.7700, 0.2500, 0.1000],\n",
       "          [0.0500, 0.8000, 0.5500]]]),\n",
       " torch.Size([2, 6, 3]))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.stack((inputs,inputs),dim =0)\n",
    "batch,batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class CasualAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_in,d_out,context_length,dropout,qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.d_out =d_out\n",
    "        self.W_query = nn.Linear(d_in,d_out,bias = qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in,d_out,bias = qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in,d_out,bias = qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "        \n",
    "    def forward(self,x):\n",
    "        b,num_tokens ,d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / math.sqrt(keys.shape[-1]), dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_lenght = batch.shape[1]\n",
    "ca = CasualAttention(d_in,d_out,context_lenght,0.0)\n",
    "context_vecs =ca(batch)  # batch shape [2, 6, 3]\n",
    "#   b,num_tokens ,d_in = x.shape [x = batch],,\n",
    "#   2  6        3\n",
    "print(context_vecs)\n",
    "print(context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]]),\n",
       " tensor([[[0.4300, 0.1500, 0.8900],\n",
       "          [0.5500, 0.8700, 0.6600],\n",
       "          [0.5700, 0.8500, 0.6400],\n",
       "          [0.2200, 0.5800, 0.3300],\n",
       "          [0.7700, 0.2500, 0.1000],\n",
       "          [0.0500, 0.8000, 0.5500]]]),\n",
       " tensor([[[0.4300, 0.1500, 0.8900],\n",
       "          [0.5500, 0.8700, 0.6600],\n",
       "          [0.5700, 0.8500, 0.6400],\n",
       "          [0.2200, 0.5800, 0.3300],\n",
       "          [0.7700, 0.2500, 0.1000],\n",
       "          [0.0500, 0.8000, 0.5500]],\n",
       " \n",
       "         [[0.4300, 0.1500, 0.8900],\n",
       "          [0.5500, 0.8700, 0.6600],\n",
       "          [0.5700, 0.8500, 0.6400],\n",
       "          [0.2200, 0.5800, 0.3300],\n",
       "          [0.7700, 0.2500, 0.1000],\n",
       "          [0.0500, 0.8000, 0.5500]]]))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0],batch[[1]],batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHead Attention --FInally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias= False):\n",
    "        super().__init__()\n",
    "        self.head=nn.ModuleList(\n",
    "        [CasualAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)])\n",
    "    def forward(self,x):\n",
    "        return torch.cat([head(x ) for head in self.head],dim =-1)\n",
    "    \n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttention(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "embed_dim must be divisible by num_heads",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20212\\388038213.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmultihead_attn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiheadAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultihead_attn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\rstudio\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_heads\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"embed_dim must be divisible by num_heads\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qkv_same_embed_dim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: embed_dim must be divisible by num_heads"
     ]
    }
   ],
   "source": [
    "multihead_attn = nn.MultiheadAttention(6,4)\n",
    "attn_output, attn_output_weights = multihead_attn(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
